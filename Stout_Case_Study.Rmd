---
title: "Stout_Case_Study"
author: "Ben Wagner"
date: "10/27/2021"
output: html_document
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(naniar)            # for analyzing missing values
library(vip)               # for variable importance plots
library(rsconnect)
library(purrr)
library(usemodels)         # for suggesting step_XXX() functions
library(glmnet)            # for regularized regression, including LASSO
library(stacks)
library(themis) 
library(modelr)
```


## Case #1

```{r}
loans <- read_csv(file = 'loans_full_schema.csv')
```

Each row of the dataset represents an individual who recieved a loan from another person via the Lending Club. The data spans 3 months (January, February, March) of 2018, and contains important statistics regarding the individuals credit when paying back this loan. I don't see any issues with the data other than the possibility that biases could be present in the predictive model of interest rate. I need to make sure that specific jobs, or underlying variables (race, gender) are not correlated with my model.

# Graph 1

```{r}
loans %>% 
  group_by(state) %>% 
  mutate(mean_ir = mean(interest_rate)) %>% 
  ggplot(aes(x= state, y= mean_ir)) +
  geom_point()+
  geom_text(aes(label=state), vjust= 1.5, size= 2)+
  labs(y= "Mean Interest Rate")+
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
  
```

You can see from this graph that the states Hawaii, North Dakota, and Connecticut are the states with the highest average interest rates. This leads me to question why those specific states have higher average interest rates. Nonetheless, there does not seem to be a clear trend between the geography of the state and interest rate.

#Graph 2

```{r}
loans %>% 
  group_by(emp_length) %>%
  mutate(mean_ir = mean(interest_rate)) %>% 
  ggplot(aes(x=emp_length, y=mean_ir))+
  geom_point()+
  geom_line()+
  labs(x="Length in Job", y= "Mean Interest Rate")+
  scale_x_continuous(breaks = (0:10))
  
```

If you look at this graph, you can see that the people who are newer to their job have higher average interest rates. This should be expected. However, people who have been at their jobs for 6 years and 8 years had unexpected high rates. This is most likely due to reasons not associated with time spent at their job, so we can assume that time on the job should be included in our model to predict interest rate.

#Graph 3

```{r}
loans %>% 
  group_by(homeownership) %>% 
  ggplot(aes(x=homeownership, y=interest_rate))+
  geom_boxplot()+
  labs(y="Interest Rate")
  
```

Taking a look at this boxplot, we see that average interest rate is highest for those who owned their homes. This is a bit of a shock because if they own the home, they will not need to pay a mortgage or rent, thus they should be more reliable in getting their loan payments payed on time. You can see though that the outliers who have high interest rates are less frequent for those people that own their house rather than renting or paying off their mortgage.

#Graph 4

```{r}
sub_grade_lm <- lm(interest_rate ~ sub_grade, loans)
loans %>% 
  #group_by(grade) %>%
  ggplot(aes(x=sub_grade, y=interest_rate)) +
  geom_point()+
  geom_abline(slope = coef(sub_grade_lm)[[2]], intercept = coef(sub_grade_lm)[[1]])+
  labs(x="Sub Grade", y="Interest Rate")
```

This graph looks at the different interest rates for each Sub Grades. I also created a linear regression to show the line of best fit for all the points on the graph. There is a strong positive trend between interest rate and sub grade. Thus Sub Grade should be a very important feature in our predictive model of interest rate.

#Graph 5

```{r}
loans %>% 
  ggplot(aes(y=interest_rate, x=account_never_delinq_percent ))+
  geom_point()+
  labs(y="Interest Rate", x="Percent of non-delinquency")
```

Noticing from this graph is yes, individuals who had a higher percent of credit lines where they were never delinquent had lower interest rates. However, those that had a lower percentage of non-delinquent lines of credit also show lower interest rates. I would think those that are more typically delinquent, should have higher levels of interest because they are not reliable in paying back their loans.


```{r}
loans2 <- loans %>% 
  filter(application_type == "individual") %>% 
  select(-application_type, -annual_income_joint, -verification_income_joint, -debt_to_income_joint)  
  

loans2
```

```{r}
loans2 %>% 
  add_n_miss() %>% 
  count(n_miss_all)
```


```{r init_split}

set.seed(327) #for reproducibility

# Randomly assigns 75% of the data to training.
loans_split <- initial_split(loans2, 
                             prop = .75)
loans_split
#<training/testing/total>

loans_training <- training(loans_split)
loans_testing <- testing(loans_split)
```

```{r recipe}

loans_recipe <- recipe(interest_rate ~ ., 
                       data = loans_training) %>% 
  # Pre-processing:
  step_rm(emp_title, state, num_accounts_120d_past_due) %>% 
  step_mutate_at(all_numeric(), fn= ~replace(., is.na(.), 0)) %>%
  #step_mutate_at(all_nominal(), fn= ~replace(., is.na(.), "Null")) %>%
  # Make these evaluative variables, not included in modeling
  update_role(all_of(c("emp_title",
                       "state",
                       "issue_month")),
              new_role = "evaluative") %>% 
  # Create indicator variables for factors/character/nominal
  # explicitly remove outcome, even though outcome isn't nominal
  # this is important in cases when we have a nominal output (eg. logistic)
  step_dummy(all_nominal(), 
             -all_outcomes(), 
             -has_role(match = "evaluative")) %>% 
  step_normalize(all_predictors(), 
                 -all_nominal())
```

```{r apply_recipe}
loans_recipe %>% 
  prep(loans_training) %>%
  juice() 
```

```{r cv}
set.seed(1211) # for reproducibility
loans_cv <- vfold_cv(loans_training, v = 5)
```

```{r lasso_mod}
loans_lasso_mod <- 
  # Define a lasso model 
  linear_reg(mixture = 1) %>% 
  # Set the engine to "glmnet" 
  set_engine("glmnet") %>% 
  # The parameters we will tune.
  set_args(penalty = tune()) %>% 
  # Use "regression"
  set_mode("regression")
```

```{r lasso_workflow}
loans_lasso_wf <- 
  # Set up the workflow
  workflow() %>% 
  # Add the recipe
  add_recipe(loans_recipe) %>% 
  # Add the modeling
  add_model(loans_lasso_mod)

loans_lasso_wf
```

```{r tune_grid}
penalty_grid <- grid_regular(penalty(),
                             levels = 20)
penalty_grid 
```

```{r tune}
loans_lasso_tune <- 
  loans_lasso_wf %>% 
  tune_grid(
    resamples = loans_cv,
    grid = penalty_grid
    )

loans_lasso_tune
```

```{r tune_results}
# The rmse for each fold:
loans_lasso_tune %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  filter(.metric == "rmse")

# rmse averaged over all folds:
loans_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") 
```

```{r tune-viz}
# Visualize rmse vs. penalty
loans_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10",scales::math_format(10^.x))) +
  labs(x = "penalty", y = "rmse")
```

```{r}
loans_lasso_tune %>% 
  show_best(metric = "rmse")
```

```{r best-tune}
# Best tuning parameter by smallest rmse
best_param <- loans_lasso_tune %>% 
  select_best(metric = "rmse")
best_param
```

```{r}
# Best tuning parameter by smallest rmse
one_se_param <- loans_lasso_tune %>% 
  select_by_one_std_err(metric = "rmse", desc(penalty))
one_se_param
```

```{r tune_wf}
loans_lasso_final_wf <- loans_lasso_wf %>% 
  finalize_workflow(one_se_param)
loans_lasso_final_wf
```

```{r lasso_train}
loans_lasso_final_mod <- loans_lasso_final_wf %>% 
  fit(data = loans_training)

loans_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy() 
```

```{r vip}
# Visualize variable importance
loans_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  vip()
```

```{r lasso_test}
# Fit model with best tuning parameter(s) to training data and apply to test data
loans_lasso_test <- loans_lasso_final_wf %>% 
  last_fit(loans_split)

# Metrics for model applied to test data
loans_lasso_test %>% 
  collect_metrics()
```

```{r act_pred_plot}
collect_predictions(loans_lasso_test) %>% 
  ggplot(aes(x = interest_rate, 
             y = .pred)) +
  geom_point(alpha = .5, 
             size = .5) +
  geom_smooth(se = FALSE) +
  geom_abline(slope = 1, 
              intercept = 0, 
              color = "darkred") +
  labs(x = "Actual Interest Rate", 
       y = "Predicted Interest Rate")
```

You can see from this graph that the predicted Interest Rates are very close to their actual rates. Thus, this is a good model for predicting our outcome variable. The approach I took was to remove all the jointly filed accounts because those observations contained too many "NA" values. Since I was using "Glmnet" to produce the model, which doesn't handle "NA" values, I simply got rid of them. In order to fill all the remaining NA values, I mutated all the numeric variables to impute a 0 for each NA. After the pre-processing was finished, I fit the model to the test set, and showed the results (variable importance, RMSE, and Actual vs. Predicted Interest).  

## Case #2

```{r}
customer_orders <- read_csv(file = 'casestudy.csv')
```

```{r}
customer_orders %>% 
  group_by(year) %>% 
  summarize(total_rev = sum(net_revenue))
```


